{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I developed a text summarization model using my own Transformer architecture. This work is mainly for practice; for real-time applications, the model requires further training. I trained it with only one-third of my dataset, specifically using 100,000 samples out of 300,000. Despite this limitation, the model performs reasonably well. If you plan to undertake a similar project, I recommend training your model on the entire dataset of 300,000 samples from the CNN/MAIL dataset to achieve better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My own Transformer architecture for summarisation task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust the number of layers in the encoder-decoder, the number of multi-head attention heads, the feedforward neural network (FFNN) dimension, and the learning rate based on your dataset size and computational resources. This model falls into the medium complexity range, but for effective generalization, it requires at least 500,000 data samples.\n",
    "\n",
    "To achieve optimal performance, carefully tune the hyperparameters and model architecture to balance accuracy and efficiency. Consider increasing the model depth and attention heads if your dataset is large enough, while ensuring that training remains feasible within your available hardware.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=256):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)  \n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  \n",
    "        pe = pe.unsqueeze(0) \n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len]\n",
    "\n",
    "###############################################\n",
    "# 2. Transformer Encoder & Decoder Layers\n",
    "###############################################\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.activation = nn.ReLU() \n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        src2, _ = self.self_attn(src, src, src, attn_mask=src_mask,\n",
    "                                   key_padding_mask=src_key_padding_mask)\n",
    "        src = self.norm1(src + self.dropout(src2))\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = self.norm2(src + self.dropout(src2))\n",
    "        return src\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.activation = nn.ReLU() \n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
    "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        tgt2, _ = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n",
    "                                 key_padding_mask=tgt_key_padding_mask)\n",
    "        tgt = self.norm1(tgt + self.dropout(tgt2))\n",
    "        tgt2, _ = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n",
    "                                      key_padding_mask=memory_key_padding_mask)\n",
    "        tgt = self.norm2(tgt + self.dropout(tgt2))\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = self.norm3(tgt + self.dropout(tgt2))\n",
    "        return tgt\n",
    "\n",
    "#################################################\n",
    "# 3. Custom Transformer Summarizer Model\n",
    "#################################################\n",
    "class TransformerSummarizer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=384, nhead=4, d_ff=1024,\n",
    "                 num_encoder_layers=3, num_decoder_layers=3, dropout=0.1,\n",
    "                 max_seq_length=256, pre_trained_embeddings=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          vocab_size: Vocabulary size.\n",
    "          d_model: Embedding dimension (should match BERTâ€™s hidden size if using its embeddings).\n",
    "          nhead: Number of attention heads.\n",
    "          d_ff: Feed-forward hidden dimension.\n",
    "          num_encoder_layers: Number of encoder layers.\n",
    "          num_decoder_layers: Number of decoder layers.\n",
    "          dropout: Dropout rate.\n",
    "          max_seq_length: Maximum sequence length (for positional encoding).\n",
    "          pre_trained_embeddings: Tensor for initializing embedding layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        if pre_trained_embeddings is not None:\n",
    "            if pre_trained_embeddings.shape == (vocab_size, d_model):\n",
    "                self.embedding.weight.data.copy_(pre_trained_embeddings)\n",
    "            else:\n",
    "                print(\"Warning: Pre-trained embedding dimensions do not match. Using random init.\")\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len=max_seq_length)\n",
    "        self.pos_decoder = PositionalEncoding(d_model, max_len=max_seq_length)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, nhead, d_ff, dropout)\n",
    "            for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(d_model, nhead, d_ff, dropout)\n",
    "            for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def encode(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        src_emb = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src_emb = self.pos_encoder(src_emb).transpose(0, 1)  \n",
    "        for layer in self.encoder_layers:\n",
    "            src_emb = layer(src_emb, src_mask, src_key_padding_mask)\n",
    "        return src_emb\n",
    "\n",
    "    def decode(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
    "               tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        tgt_emb = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt_emb = self.pos_decoder(tgt_emb).transpose(0, 1)  \n",
    "        for layer in self.decoder_layers:\n",
    "            tgt_emb = layer(tgt_emb, memory, tgt_mask, memory_mask,\n",
    "                            tgt_key_padding_mask, memory_key_padding_mask)\n",
    "        return tgt_emb\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None,\n",
    "                src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          src: Source tokens (batch_size, src_seq_len)\n",
    "          tgt: Target tokens (batch_size, tgt_seq_len)\n",
    "        Returns:\n",
    "          Logits over vocabulary for each target token.\n",
    "          Shape: (tgt_seq_len, batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "        memory = self.encode(src, src_mask, src_key_padding_mask)\n",
    "        decoder_output = self.decode(tgt, memory, tgt_mask, None,\n",
    "                                     tgt_key_padding_mask, src_key_padding_mask)\n",
    "        return self.output_layer(decoder_output)\n",
    "\n",
    "    def generate(self, src, src_mask=None, src_key_padding_mask=None,\n",
    "                 max_length=100, beam_width=5, start_token_id=101, end_token_id=102, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Generate summary using beam search.\n",
    "        Args:\n",
    "          src: Source tensor (1, src_seq_len)\n",
    "          start_token_id: Start token (e.g., [CLS] for BERT is 101).\n",
    "          end_token_id: End token (e.g., [SEP] for BERT is 102).\n",
    "        Returns:\n",
    "          List of token IDs representing the generated summary.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            memory = self.encode(src, src_mask, src_key_padding_mask)\n",
    "            beams = [([start_token_id], 0.0)]\n",
    "            for _ in range(max_length):\n",
    "                new_beams = []\n",
    "                for seq, score in beams:\n",
    "                    if seq[-1] == end_token_id:\n",
    "                        new_beams.append((seq, score))\n",
    "                        continue\n",
    "                    tgt_seq = torch.tensor(seq, dtype=torch.long, device=device).unsqueeze(0)\n",
    "                    seq_len = tgt_seq.size(1)\n",
    "                    tgt_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(device)\n",
    "                    decoder_output = self.decode(tgt_seq, memory, tgt_mask=tgt_mask)\n",
    "                    last_output = decoder_output[-1, :, :] \n",
    "                    logits = self.output_layer(last_output)  \n",
    "                    log_probs = torch.log_softmax(logits, dim=-1).squeeze(0)\n",
    "                    top_log_probs, top_indices = torch.topk(log_probs, beam_width)\n",
    "                    for log_p, token_id in zip(top_log_probs.tolist(), top_indices.tolist()):\n",
    "                        new_seq = seq + [token_id]\n",
    "                        new_score = score + log_p\n",
    "                        new_beams.append((new_seq, new_score))\n",
    "                beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "                if all(seq[-1] == end_token_id for seq, _ in beams):\n",
    "                    break\n",
    "            best_sequence = beams[0][0]\n",
    "            return best_sequence\n",
    "\n",
    "#################################################\n",
    "# 4. Custom Dataset Class for Summarization Data\n",
    "#################################################\n",
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_src_len=512, max_tgt_len=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          csv_file: Path to CSV file with 'article' and 'highlights' columns.\n",
    "          tokenizer: Pre-trained tokenizer (BERT in our case).\n",
    "          max_src_len: Maximum token length for the source article.\n",
    "          max_tgt_len: Maximum token length for the target summary.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_src_len = max_src_len\n",
    "        self.max_tgt_len = max_tgt_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        article = str(row[\"article\"])\n",
    "        summary = str(row[\"highlights\"])\n",
    "        src_encoding = self.tokenizer(article, max_length=self.max_src_len,\n",
    "                                      truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        tgt_encoding = self.tokenizer(summary, max_length=self.max_tgt_len,\n",
    "                                      truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        src_ids = src_encoding[\"input_ids\"].squeeze(0)\n",
    "        tgt_ids = tgt_encoding[\"input_ids\"].squeeze(0)\n",
    "        return {\"src\": src_ids, \"tgt\": tgt_ids}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Loading BERT tokenizer and model for embeddings...\")\n",
    "    bert_model_name = \"bert-base-uncased\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "    bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "    bert_model.eval() \n",
    "    bert_embeddings = bert_model.embeddings.word_embeddings.weight.data.clone()\n",
    "\n",
    "    vocab_size = tokenizer.vocab_size  \n",
    "    d_model = bert_embeddings.size(1)   \n",
    "    nhead = 4\n",
    "    d_ff = 1024\n",
    "    num_encoder_layers = 3\n",
    "    num_decoder_layers = 3\n",
    "    dropout = 0.1\n",
    "    max_seq_length = 256  \n",
    "    batch_size = 64    \n",
    "    num_epochs = 10    \n",
    "\n",
    "    print(\"Initializing custom Transformer summarization model...\")\n",
    "    model = TransformerSummarizer(\n",
    "        vocab_size=vocab_size,\n",
    "        d_model=d_model,\n",
    "        nhead=nhead,\n",
    "        d_ff=d_ff,\n",
    "        num_encoder_layers=num_encoder_layers,\n",
    "        num_decoder_layers=num_decoder_layers,\n",
    "        dropout=dropout,\n",
    "        max_seq_length=max_seq_length,\n",
    "        pre_trained_embeddings=bert_embeddings\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "    train_dataset = SummarizationDataset(\"/content/part1.csv\", tokenizer, max_src_len=max_seq_length, max_tgt_len=128)\n",
    "    val_dataset   = SummarizationDataset(\"/content/validation.csv\", tokenizer, max_src_len=max_seq_length, max_tgt_len=128)\n",
    "    test_dataset  = SummarizationDataset(\"/content/test.csv\", tokenizer, max_src_len=max_seq_length, max_tgt_len=128)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=1, shuffle=False)  \n",
    "    ##########################################################\n",
    "    # 6. Training and Validation Loop\n",
    "    ##########################################################\n",
    "    best_val_loss = float(\"inf\")\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch}\"):\n",
    "            src = batch[\"src\"].to(device)  \n",
    "            tgt = batch[\"tgt\"].to(device)  \n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_labels = tgt[:, 1:]\n",
    "            tgt_seq_len = tgt_input.size(1)\n",
    "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_seq_len).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output_logits = model(src, tgt_input, tgt_mask=tgt_mask)\n",
    "            loss = criterion(output_logits.view(-1, vocab_size), tgt_labels.transpose(0, 1).reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch} Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch}\"):\n",
    "                src = batch[\"src\"].to(device)\n",
    "                tgt = batch[\"tgt\"].to(device)\n",
    "                tgt_input = tgt[:, :-1]\n",
    "                tgt_labels = tgt[:, 1:]\n",
    "                tgt_seq_len = tgt_input.size(1)\n",
    "                tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_seq_len).to(device)\n",
    "                output_logits = model(src, tgt_input, tgt_mask=tgt_mask)\n",
    "                loss = criterion(output_logits.view(-1, vocab_size), tgt_labels.transpose(0, 1).reshape(-1))\n",
    "                val_loss += loss.item()\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch} Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_path = \"best_transformer_summarizer.pt\"\n",
    "            torch.save({\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"model_config\": {\n",
    "                    \"vocab_size\": vocab_size,\n",
    "                    \"d_model\": d_model,\n",
    "                    \"nhead\": nhead,\n",
    "                    \"d_ff\": d_ff,\n",
    "                    \"num_encoder_layers\": num_encoder_layers,\n",
    "                    \"num_decoder_layers\": num_decoder_layers,\n",
    "                    \"dropout\": dropout,\n",
    "                    \"max_seq_length\": max_seq_length,\n",
    "                    \"bert_model_name\": bert_model_name\n",
    "                }\n",
    "            }, best_model_path)\n",
    "            print(f\"Saved best model at epoch {epoch} to {best_model_path}\")\n",
    "\n",
    "    ##########################################################\n",
    "    # 7. Testing / Generation on Test Dataset\n",
    "    ##########################################################\n",
    "    print(\"Generating summaries on test dataset...\")\n",
    "    model.eval()\n",
    "    results = []\n",
    "    for i, batch in enumerate(tqdm(test_loader, desc=\"Testing\")):\n",
    "        src = batch[\"src\"].to(device)  \n",
    "        generated_ids = model.generate(src, max_length=50, beam_width=5,\n",
    "                                       start_token_id=tokenizer.cls_token_id,\n",
    "                                       end_token_id=tokenizer.sep_token_id,\n",
    "                                       device=device)\n",
    "        generated_summary = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "        results.append(generated_summary)\n",
    "\n",
    "    # Save test results to a CSV file.\n",
    "    test_results_df = pd.DataFrame({\"generated_summary\": results})\n",
    "    test_results_df.to_csv(\"test_generated_summaries.csv\", index=False)\n",
    "    print(\"Test summaries saved to test_generated_summaries.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
